{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca27b07a-dadb-4228-89f0-d7bf01790808",
   "metadata": {},
   "source": [
    "**This demonstrates Video Object Detection using two methods**:-\n",
    "\n",
    "- YOLOv5 (modern deep learning)\n",
    "- MobileNet SSD (lightweight deep learning)\n",
    "  \n",
    "**we will**:-\n",
    "\n",
    "- Install dependencies\n",
    "- Load video\n",
    "- Run each model\n",
    "- Save annotated videos, per‑class detection counts, and FPS.\n",
    "- Produce a comparison table (YOLOv5 vs MobileNet‑SSD).\n",
    "- A summary PPT\n",
    "- YOLOv5 detection\n",
    "- MobileNet SSD detection\n",
    "- Annotated video outputs\n",
    "- Compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ab6f4d-2564-4677-a48a-dbdfa9ce35b7",
   "metadata": {},
   "source": [
    "# AI Video Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b79cb6-064a-4596-9850-57468604ab62",
   "metadata": {},
   "source": [
    "## Step 1: Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8228d3b4-83dd-45b4-9765-6354e14ba192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\prasu\\anaconda3\\lib\\site-packages (4.12.0.88)\n",
      "Requirement already satisfied: torch in c:\\users\\prasu\\anaconda3\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\prasu\\anaconda3\\lib\\site-packages (0.23.0)\n",
      "Requirement already satisfied: ultralytics in c:\\users\\prasu\\anaconda3\\lib\\site-packages (8.3.179)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from opencv-python) (2.1.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from ultralytics) (3.10.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from ultralytics) (1.15.3)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from ultralytics) (5.9.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from ultralytics) (2.0.15)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\prasu\\anaconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python torch torchvision ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ee9f1f-99d7-4729-93fe-eb84847e6ad4",
   "metadata": {},
   "source": [
    "## Step 2: Load Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eec7ba4-48d8-4657-8a96-086d8c76ad0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11b5ed76-eeaa-4c9d-aa24-e660ba56a87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Path to input video\n",
    "video_path = \"prasu_video.mp4\"\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "else:\n",
    "    print(\"Video loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e648741a-f060-4969-a55a-9d27688b1c95",
   "metadata": {},
   "source": [
    "## Step 3: Run each model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f460686-125e-4944-93cb-a6c99057aceb",
   "metadata": {},
   "source": [
    "## AI Video Object Detection using YOLOv5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad35a051-91fc-419c-8c9e-6bec3bbe4691",
   "metadata": {},
   "source": [
    "# Model 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bed64f0-88d2-4284-8773-aa5e11996537",
   "metadata": {},
   "source": [
    "### Import Libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d11b377-bf08-4e1f-95fd-16b6117f3e0e",
   "metadata": {},
   "source": [
    "- Import YOLOv5 and OpenCV libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18a7e5e9-fd52-45cd-b9d5-f8e17d0493d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2, time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d37a52e-26cb-4b12-acdd-62eae4eb86ed",
   "metadata": {},
   "source": [
    "### Load YOLOv5 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23361f78-3887-48b7-8c0c-c3b480b96572",
   "metadata": {},
   "source": [
    "- Load the pre-trained `yolov5s.pt` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bee4319-470f-4c72-b1db-923e33039ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRO TIP  Replace 'model=yolov5s.pt' with new 'model=yolov5su.pt'.\n",
      "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('yolov5s.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03d4778-448b-49f2-ab49-42f6e9f6ab1a",
   "metadata": {},
   "source": [
    "### Test on Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cddb522-a7b9-426f-a008-461b709aa50d",
   "metadata": {},
   "source": [
    "- Test YOLOv5 on a sample image like `bus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87902125-fb49-43ff-a285-567a2bc204d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
      "image 1/1 C:\\Users\\prasu\\bus.jpg: 640x480 4 persons, 1 bus, 310.1ms\n",
      "Speed: 4.3ms preprocess, 310.1ms inference, 3.2ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    }
   ],
   "source": [
    "results = model('https://ultralytics.com/images/bus.jpg')\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b2f786-26f3-4137-8926-5a362e396021",
   "metadata": {},
   "source": [
    "### Run YOLOv5 on Video (prasu_video.mp4) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61589507-0950-46ea-af4e-1f51dda2ce8d",
   "metadata": {},
   "source": [
    "- Now YOLOv5 will detect objects in our video and draw bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "977d78f8-431a-4e7a-9e6c-fce363999c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 person, 197.1ms\n",
      "Speed: 2.5ms preprocess, 197.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 172.5ms\n",
      "Speed: 3.2ms preprocess, 172.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 197.4ms\n",
      "Speed: 2.7ms preprocess, 197.4ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 190.3ms\n",
      "Speed: 2.2ms preprocess, 190.3ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 191.2ms\n",
      "Speed: 2.2ms preprocess, 191.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 193.8ms\n",
      "Speed: 2.8ms preprocess, 193.8ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 193.1ms\n",
      "Speed: 3.4ms preprocess, 193.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 196.7ms\n",
      "Speed: 2.5ms preprocess, 196.7ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 233.3ms\n",
      "Speed: 2.4ms preprocess, 233.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 190.7ms\n",
      "Speed: 2.8ms preprocess, 190.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 200.6ms\n",
      "Speed: 2.2ms preprocess, 200.6ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 228.2ms\n",
      "Speed: 3.1ms preprocess, 228.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 231.8ms\n",
      "Speed: 2.0ms preprocess, 231.8ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 218.8ms\n",
      "Speed: 3.0ms preprocess, 218.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 207.1ms\n",
      "Speed: 3.3ms preprocess, 207.1ms inference, 3.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 421.2ms\n",
      "Speed: 5.2ms preprocess, 421.2ms inference, 3.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 370.9ms\n",
      "Speed: 5.8ms preprocess, 370.9ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 353.7ms\n",
      "Speed: 6.1ms preprocess, 353.7ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 342.2ms\n",
      "Speed: 5.3ms preprocess, 342.2ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 316.6ms\n",
      "Speed: 5.0ms preprocess, 316.6ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 350.4ms\n",
      "Speed: 5.4ms preprocess, 350.4ms inference, 5.3ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 353.1ms\n",
      "Speed: 5.7ms preprocess, 353.1ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 330.0ms\n",
      "Speed: 5.3ms preprocess, 330.0ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 319.7ms\n",
      "Speed: 5.2ms preprocess, 319.7ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 315.9ms\n",
      "Speed: 5.2ms preprocess, 315.9ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 318.2ms\n",
      "Speed: 5.1ms preprocess, 318.2ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 315.7ms\n",
      "Speed: 5.4ms preprocess, 315.7ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 313.4ms\n",
      "Speed: 5.1ms preprocess, 313.4ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 326.0ms\n",
      "Speed: 5.3ms preprocess, 326.0ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 328.3ms\n",
      "Speed: 5.5ms preprocess, 328.3ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 320.7ms\n",
      "Speed: 4.9ms preprocess, 320.7ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 299.6ms\n",
      "Speed: 4.7ms preprocess, 299.6ms inference, 3.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 328.8ms\n",
      "Speed: 5.1ms preprocess, 328.8ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 322.9ms\n",
      "Speed: 5.2ms preprocess, 322.9ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 368.1ms\n",
      "Speed: 5.9ms preprocess, 368.1ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 371.9ms\n",
      "Speed: 5.2ms preprocess, 371.9ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 369.9ms\n",
      "Speed: 3.1ms preprocess, 369.9ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 376.6ms\n",
      "Speed: 5.1ms preprocess, 376.6ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 383.6ms\n",
      "Speed: 5.3ms preprocess, 383.6ms inference, 3.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 377.3ms\n",
      "Speed: 5.2ms preprocess, 377.3ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 378.7ms\n",
      "Speed: 5.1ms preprocess, 378.7ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 381.0ms\n",
      "Speed: 5.2ms preprocess, 381.0ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 324.0ms\n",
      "Speed: 5.0ms preprocess, 324.0ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 325.0ms\n",
      "Speed: 4.9ms preprocess, 325.0ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 308.3ms\n",
      "Speed: 4.9ms preprocess, 308.3ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 312.5ms\n",
      "Speed: 4.9ms preprocess, 312.5ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 310.8ms\n",
      "Speed: 5.0ms preprocess, 310.8ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 313.1ms\n",
      "Speed: 5.1ms preprocess, 313.1ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 324.6ms\n",
      "Speed: 4.8ms preprocess, 324.6ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 314.4ms\n",
      "Speed: 4.9ms preprocess, 314.4ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 316.3ms\n",
      "Speed: 5.1ms preprocess, 316.3ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 313.6ms\n",
      "Speed: 5.0ms preprocess, 313.6ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 306.3ms\n",
      "Speed: 5.1ms preprocess, 306.3ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 313.2ms\n",
      "Speed: 5.1ms preprocess, 313.2ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 320.8ms\n",
      "Speed: 4.9ms preprocess, 320.8ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 311.5ms\n",
      "Speed: 5.0ms preprocess, 311.5ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 316.9ms\n",
      "Speed: 4.9ms preprocess, 316.9ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 306.8ms\n",
      "Speed: 4.9ms preprocess, 306.8ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 310.5ms\n",
      "Speed: 5.2ms preprocess, 310.5ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 320.0ms\n",
      "Speed: 4.9ms preprocess, 320.0ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 339.6ms\n",
      "Speed: 5.3ms preprocess, 339.6ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 315.2ms\n",
      "Speed: 4.9ms preprocess, 315.2ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 327.9ms\n",
      "Speed: 5.0ms preprocess, 327.9ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 337.7ms\n",
      "Speed: 5.0ms preprocess, 337.7ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 1 potted plant, 307.9ms\n",
      "Speed: 4.9ms preprocess, 307.9ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 1 potted plant, 315.3ms\n",
      "Speed: 4.9ms preprocess, 315.3ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 347.3ms\n",
      "Speed: 5.0ms preprocess, 347.3ms inference, 4.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 354.5ms\n",
      "Speed: 5.0ms preprocess, 354.5ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 368.3ms\n",
      "Speed: 5.1ms preprocess, 368.3ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 383.8ms\n",
      "Speed: 5.1ms preprocess, 383.8ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 394.8ms\n",
      "Speed: 5.1ms preprocess, 394.8ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 315.0ms\n",
      "Speed: 5.0ms preprocess, 315.0ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 346.1ms\n",
      "Speed: 5.0ms preprocess, 346.1ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 321.0ms\n",
      "Speed: 4.9ms preprocess, 321.0ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 1 potted plant, 314.1ms\n",
      "Speed: 4.9ms preprocess, 314.1ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 321.3ms\n",
      "Speed: 4.9ms preprocess, 321.3ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 308.6ms\n",
      "Speed: 5.0ms preprocess, 308.6ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 323.1ms\n",
      "Speed: 4.7ms preprocess, 323.1ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 307.3ms\n",
      "Speed: 5.0ms preprocess, 307.3ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 314.7ms\n",
      "Speed: 5.1ms preprocess, 314.7ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 337.0ms\n",
      "Speed: 4.9ms preprocess, 337.0ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 310.8ms\n",
      "Speed: 5.0ms preprocess, 310.8ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 327.3ms\n",
      "Speed: 4.9ms preprocess, 327.3ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 321.4ms\n",
      "Speed: 5.0ms preprocess, 321.4ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 305.9ms\n",
      "Speed: 4.9ms preprocess, 305.9ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 316.9ms\n",
      "Speed: 5.1ms preprocess, 316.9ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 342.8ms\n",
      "Speed: 4.9ms preprocess, 342.8ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 310.6ms\n",
      "Speed: 5.0ms preprocess, 310.6ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 369.7ms\n",
      "Speed: 4.9ms preprocess, 369.7ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 316.2ms\n",
      "Speed: 5.3ms preprocess, 316.2ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 318.0ms\n",
      "Speed: 5.1ms preprocess, 318.0ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 316.5ms\n",
      "Speed: 4.9ms preprocess, 316.5ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 305.3ms\n",
      "Speed: 4.9ms preprocess, 305.3ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 318.5ms\n",
      "Speed: 5.1ms preprocess, 318.5ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 337.3ms\n",
      "Speed: 4.9ms preprocess, 337.3ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 313.0ms\n",
      "Speed: 5.0ms preprocess, 313.0ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 313.5ms\n",
      "Speed: 5.0ms preprocess, 313.5ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 319.7ms\n",
      "Speed: 5.0ms preprocess, 319.7ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 324.5ms\n",
      "Speed: 5.0ms preprocess, 324.5ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 365.1ms\n",
      "Speed: 4.9ms preprocess, 365.1ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 377.1ms\n",
      "Speed: 5.0ms preprocess, 377.1ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 379.9ms\n",
      "Speed: 5.2ms preprocess, 379.9ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 370.4ms\n",
      "Speed: 5.4ms preprocess, 370.4ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 315.7ms\n",
      "Speed: 5.2ms preprocess, 315.7ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 316.4ms\n",
      "Speed: 5.0ms preprocess, 316.4ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 322.0ms\n",
      "Speed: 5.4ms preprocess, 322.0ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 313.5ms\n",
      "Speed: 5.0ms preprocess, 313.5ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 315.1ms\n",
      "Speed: 5.2ms preprocess, 315.1ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 313.1ms\n",
      "Speed: 5.0ms preprocess, 313.1ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 309.1ms\n",
      "Speed: 5.1ms preprocess, 309.1ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 343.3ms\n",
      "Speed: 5.1ms preprocess, 343.3ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 318.3ms\n",
      "Speed: 5.0ms preprocess, 318.3ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 314.0ms\n",
      "Speed: 4.8ms preprocess, 314.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 308.1ms\n",
      "Speed: 4.9ms preprocess, 308.1ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 311.0ms\n",
      "Speed: 5.0ms preprocess, 311.0ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 312.6ms\n",
      "Speed: 4.9ms preprocess, 312.6ms inference, 3.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 329.8ms\n",
      "Speed: 5.1ms preprocess, 329.8ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 372.0ms\n",
      "Speed: 5.2ms preprocess, 372.0ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 361.2ms\n",
      "Speed: 5.2ms preprocess, 361.2ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 307.8ms\n",
      "Speed: 4.8ms preprocess, 307.8ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 339.5ms\n",
      "Speed: 5.0ms preprocess, 339.5ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 316.8ms\n",
      "Speed: 5.0ms preprocess, 316.8ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 324.2ms\n",
      "Speed: 4.9ms preprocess, 324.2ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 312.3ms\n",
      "Speed: 4.9ms preprocess, 312.3ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 347.6ms\n",
      "Speed: 5.0ms preprocess, 347.6ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 309.2ms\n",
      "Speed: 5.3ms preprocess, 309.2ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 309.9ms\n",
      "Speed: 4.9ms preprocess, 309.9ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 310.9ms\n",
      "Speed: 5.0ms preprocess, 310.9ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 314.4ms\n",
      "Speed: 4.9ms preprocess, 314.4ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 318.0ms\n",
      "Speed: 4.8ms preprocess, 318.0ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 310.9ms\n",
      "Speed: 5.2ms preprocess, 310.9ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 329.3ms\n",
      "Speed: 4.8ms preprocess, 329.3ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 375.6ms\n",
      "Speed: 5.3ms preprocess, 375.6ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 385.1ms\n",
      "Speed: 5.1ms preprocess, 385.1ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 371.9ms\n",
      "Speed: 5.2ms preprocess, 371.9ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 344.4ms\n",
      "Speed: 5.1ms preprocess, 344.4ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 308.0ms\n",
      "Speed: 5.0ms preprocess, 308.0ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 320.0ms\n",
      "Speed: 5.0ms preprocess, 320.0ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 342.5ms\n",
      "Speed: 4.9ms preprocess, 342.5ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 319.5ms\n",
      "Speed: 5.2ms preprocess, 319.5ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 312.0ms\n",
      "Speed: 5.0ms preprocess, 312.0ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 316.4ms\n",
      "Speed: 5.0ms preprocess, 316.4ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 310.5ms\n",
      "Speed: 5.1ms preprocess, 310.5ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 332.9ms\n",
      "Speed: 5.1ms preprocess, 332.9ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 516.7ms\n",
      "Speed: 14.2ms preprocess, 516.7ms inference, 3.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 1 vase, 411.5ms\n",
      "Speed: 5.1ms preprocess, 411.5ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 424.3ms\n",
      "Speed: 5.1ms preprocess, 424.3ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 413.4ms\n",
      "Speed: 6.0ms preprocess, 413.4ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 365.5ms\n",
      "Speed: 5.3ms preprocess, 365.5ms inference, 3.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 341.6ms\n",
      "Speed: 4.9ms preprocess, 341.6ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 310.2ms\n",
      "Speed: 4.9ms preprocess, 310.2ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 424.3ms\n",
      "Speed: 5.1ms preprocess, 424.3ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 340.1ms\n",
      "Speed: 4.9ms preprocess, 340.1ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 322.8ms\n",
      "Speed: 5.3ms preprocess, 322.8ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 336.4ms\n",
      "Speed: 5.1ms preprocess, 336.4ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 323.3ms\n",
      "Speed: 4.9ms preprocess, 323.3ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 362.2ms\n",
      "Speed: 5.4ms preprocess, 362.2ms inference, 3.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 369.5ms\n",
      "Speed: 4.9ms preprocess, 369.5ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 353.2ms\n",
      "Speed: 4.9ms preprocess, 353.2ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 340.5ms\n",
      "Speed: 5.1ms preprocess, 340.5ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 318.0ms\n",
      "Speed: 5.3ms preprocess, 318.0ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 358.4ms\n",
      "Speed: 5.0ms preprocess, 358.4ms inference, 4.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 381.3ms\n",
      "Speed: 5.2ms preprocess, 381.3ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 376.0ms\n",
      "Speed: 5.1ms preprocess, 376.0ms inference, 3.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 379.7ms\n",
      "Speed: 5.1ms preprocess, 379.7ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 317.1ms\n",
      "Speed: 5.2ms preprocess, 317.1ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 368.9ms\n",
      "Speed: 5.1ms preprocess, 368.9ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 338.4ms\n",
      "Speed: 4.9ms preprocess, 338.4ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 363.4ms\n",
      "Speed: 4.9ms preprocess, 363.4ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 345.5ms\n",
      "Speed: 5.1ms preprocess, 345.5ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 350.5ms\n",
      "Speed: 5.1ms preprocess, 350.5ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 321.4ms\n",
      "Speed: 4.9ms preprocess, 321.4ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 357.9ms\n",
      "Speed: 4.8ms preprocess, 357.9ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 314.2ms\n",
      "Speed: 5.1ms preprocess, 314.2ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 321.3ms\n",
      "Speed: 5.0ms preprocess, 321.3ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 293.3ms\n",
      "Speed: 5.0ms preprocess, 293.3ms inference, 3.2ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 291.5ms\n",
      "Speed: 4.7ms preprocess, 291.5ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 356.5ms\n",
      "Speed: 4.8ms preprocess, 356.5ms inference, 4.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 344.6ms\n",
      "Speed: 5.0ms preprocess, 344.6ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 308.1ms\n",
      "Speed: 5.0ms preprocess, 308.1ms inference, 3.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 336.2ms\n",
      "Speed: 5.0ms preprocess, 336.2ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 328.0ms\n",
      "Speed: 5.2ms preprocess, 328.0ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 316.2ms\n",
      "Speed: 4.9ms preprocess, 316.2ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 307.3ms\n",
      "Speed: 5.0ms preprocess, 307.3ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 314.5ms\n",
      "Speed: 5.2ms preprocess, 314.5ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 305.3ms\n",
      "Speed: 4.8ms preprocess, 305.3ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 1 tie, 1 vase, 314.0ms\n",
      "Speed: 5.0ms preprocess, 314.0ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 1 tie, 331.8ms\n",
      "Speed: 5.0ms preprocess, 331.8ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 373.1ms\n",
      "Speed: 5.0ms preprocess, 373.1ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 317.6ms\n",
      "Speed: 4.9ms preprocess, 317.6ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 304.2ms\n",
      "Speed: 4.9ms preprocess, 304.2ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 316.8ms\n",
      "Speed: 5.1ms preprocess, 316.8ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 312.9ms\n",
      "Speed: 4.9ms preprocess, 312.9ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 364.9ms\n",
      "Speed: 4.9ms preprocess, 364.9ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 367.2ms\n",
      "Speed: 5.4ms preprocess, 367.2ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 369.8ms\n",
      "Speed: 5.3ms preprocess, 369.8ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 381.7ms\n",
      "Speed: 5.1ms preprocess, 381.7ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 335.8ms\n",
      "Speed: 5.1ms preprocess, 335.8ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 344.8ms\n",
      "Speed: 5.1ms preprocess, 344.8ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 313.8ms\n",
      "Speed: 5.0ms preprocess, 313.8ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 315.6ms\n",
      "Speed: 5.2ms preprocess, 315.6ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 314.9ms\n",
      "Speed: 5.3ms preprocess, 314.9ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 353.7ms\n",
      "Speed: 4.9ms preprocess, 353.7ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 326.4ms\n",
      "Speed: 4.9ms preprocess, 326.4ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 315.4ms\n",
      "Speed: 4.9ms preprocess, 315.4ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 308.9ms\n",
      "Speed: 5.2ms preprocess, 308.9ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 1 tie, 312.7ms\n",
      "Speed: 5.0ms preprocess, 312.7ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 309.7ms\n",
      "Speed: 4.9ms preprocess, 309.7ms inference, 4.3ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 303.9ms\n",
      "Speed: 4.9ms preprocess, 303.9ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 314.5ms\n",
      "Speed: 5.0ms preprocess, 314.5ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 305.1ms\n",
      "Speed: 5.1ms preprocess, 305.1ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 314.1ms\n",
      "Speed: 4.9ms preprocess, 314.1ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 328.6ms\n",
      "Speed: 5.0ms preprocess, 328.6ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 312.4ms\n",
      "Speed: 5.2ms preprocess, 312.4ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 312.9ms\n",
      "Speed: 5.0ms preprocess, 312.9ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 317.4ms\n",
      "Speed: 5.0ms preprocess, 317.4ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 309.8ms\n",
      "Speed: 5.0ms preprocess, 309.8ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 308.0ms\n",
      "Speed: 5.0ms preprocess, 308.0ms inference, 3.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 313.1ms\n",
      "Speed: 5.1ms preprocess, 313.1ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 326.1ms\n",
      "Speed: 4.9ms preprocess, 326.1ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 311.4ms\n",
      "Speed: 4.9ms preprocess, 311.4ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 314.4ms\n",
      "Speed: 5.0ms preprocess, 314.4ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 314.5ms\n",
      "Speed: 5.1ms preprocess, 314.5ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 310.3ms\n",
      "Speed: 5.0ms preprocess, 310.3ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 316.8ms\n",
      "Speed: 5.1ms preprocess, 316.8ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 316.7ms\n",
      "Speed: 5.2ms preprocess, 316.7ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 358.3ms\n",
      "Speed: 4.9ms preprocess, 358.3ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 381.3ms\n",
      "Speed: 5.1ms preprocess, 381.3ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 386.9ms\n",
      "Speed: 5.3ms preprocess, 386.9ms inference, 3.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 377.8ms\n",
      "Speed: 5.2ms preprocess, 377.8ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 327.6ms\n",
      "Speed: 4.9ms preprocess, 327.6ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 314.4ms\n",
      "Speed: 5.1ms preprocess, 314.4ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 336.7ms\n",
      "Speed: 5.0ms preprocess, 336.7ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 311.6ms\n",
      "Speed: 5.0ms preprocess, 311.6ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 309.7ms\n",
      "Speed: 5.0ms preprocess, 309.7ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 326.5ms\n",
      "Speed: 5.2ms preprocess, 326.5ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 316.6ms\n",
      "Speed: 5.1ms preprocess, 316.6ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 322.2ms\n",
      "Speed: 5.0ms preprocess, 322.2ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 320.6ms\n",
      "Speed: 5.1ms preprocess, 320.6ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 312.7ms\n",
      "Speed: 5.2ms preprocess, 312.7ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 311.4ms\n",
      "Speed: 5.0ms preprocess, 311.4ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 311.8ms\n",
      "Speed: 5.1ms preprocess, 311.8ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 333.9ms\n",
      "Speed: 5.2ms preprocess, 333.9ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 323.8ms\n",
      "Speed: 5.1ms preprocess, 323.8ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 327.7ms\n",
      "Speed: 5.1ms preprocess, 327.7ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 317.0ms\n",
      "Speed: 5.1ms preprocess, 317.0ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 317.1ms\n",
      "Speed: 5.0ms preprocess, 317.1ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 332.5ms\n",
      "Speed: 4.9ms preprocess, 332.5ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 348.0ms\n",
      "Speed: 4.9ms preprocess, 348.0ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 313.5ms\n",
      "Speed: 5.2ms preprocess, 313.5ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 322.1ms\n",
      "Speed: 4.8ms preprocess, 322.1ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 307.4ms\n",
      "Speed: 4.9ms preprocess, 307.4ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 1 tie, 319.6ms\n",
      "Speed: 5.0ms preprocess, 319.6ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 312.0ms\n",
      "Speed: 4.7ms preprocess, 312.0ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 312.3ms\n",
      "Speed: 5.0ms preprocess, 312.3ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 317.5ms\n",
      "Speed: 5.0ms preprocess, 317.5ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 326.7ms\n",
      "Speed: 5.0ms preprocess, 326.7ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 312.7ms\n",
      "Speed: 5.2ms preprocess, 312.7ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 355.1ms\n",
      "Speed: 5.2ms preprocess, 355.1ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 370.9ms\n",
      "Speed: 5.2ms preprocess, 370.9ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 380.1ms\n",
      "Speed: 5.2ms preprocess, 380.1ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      " YOLOv5 Annotated Video Saved as: prasu_video_yolo.mp4\n",
      "YOLOv5 Detections: {'person': 261, 'potted plant': 3, 'vase': 2, 'tie': 4}\n",
      "YOLOv5 FPS: 2.82\n"
     ]
    }
   ],
   "source": [
    "# load the our video\n",
    "input_video = 'prasu_video.mp4'\n",
    "output_video = 'prasu_video_yolo.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(input_video)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n",
    "\n",
    "yolo_detections_count = {}\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    results = model(frame)\n",
    "    annotated_frame = results[0].plot()\n",
    "    \n",
    "    for box in results[0].boxes.data:\n",
    "        cls_id = int(box[5])\n",
    "        cls_name = model.names[cls_id]\n",
    "        yolo_detections_count[cls_name] = yolo_detections_count.get(cls_name, 0) + 1\n",
    "    \n",
    "    out.write(annotated_frame)\n",
    "\n",
    "end_time = time.time()\n",
    "yolo_fps = cap.get(cv2.CAP_PROP_FRAME_COUNT) / (end_time - start_time)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "print(' YOLOv5 Annotated Video Saved as:', output_video)\n",
    "print(\"YOLOv5 Detections:\", yolo_detections_count)\n",
    "print(f\"YOLOv5 FPS: {yolo_fps:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b045530-ac02-48c5-acf3-ad81da136412",
   "metadata": {},
   "source": [
    "- **YOLOv5 Detections**:- {` 4 `} \n",
    "- **YOLOv5 FPS**(Frames of detection Per Second):- `2.82`\n",
    "- **Speed**: `5.2ms preprocess`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a394d227-1793-4031-94a4-30b3fb942145",
   "metadata": {},
   "source": [
    "#  YOLOv5\n",
    "\n",
    "1. Detected `4` unique object categories \n",
    "\n",
    "2. Lower FPS (`2.8`), but it actually worked correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf7bc46-ec66-4792-83c1-2a637371d7ef",
   "metadata": {},
   "source": [
    "### Conclusion :\n",
    "- YOLOv5 has now detected objects in the video and saved the annotated version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59dcf8f-5f77-437e-b204-aec506d93a03",
   "metadata": {},
   "source": [
    "## MobileNet SSD Video Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4109c72d-930a-41c1-b99b-269ac98424aa",
   "metadata": {},
   "source": [
    "# Model 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd88833-7f4f-4920-85b3-44a97a4c1961",
   "metadata": {},
   "source": [
    "**Requirements** :\n",
    "- You need OpenCV with DNN module enabled. We'll import it directly below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4768c2e8-938c-46ad-9e1e-3e31fdeee4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV version: 4.12.0\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import cv2, os, time, numpy as np\n",
    "from pathlib import Path\n",
    "print('OpenCV version:', cv2.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724441c2-280e-44ea-84ed-3539f1890d9f",
   "metadata": {},
   "source": [
    "## Model Files\n",
    "- prototxt and caffemodel  -Keep both files in your working directory, like upload in our jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3560a04-e699-4da3-80ea-8fb37a39326d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found model files.\n"
     ]
    }
   ],
   "source": [
    "# loaded these files\n",
    "PROTOTXT = 'MobileNetSSD_deploy.prototxt'\n",
    "CAFFEMODEL = 'MobileNetSSD_deploy.caffemodel'\n",
    "\n",
    "if not (os.path.exists(PROTOTXT) and os.path.exists(CAFFEMODEL)):\n",
    "    raise FileNotFoundError(\n",
    "        'Model files not found. Please place MobileNetSSD_deploy.prototxt and '\n",
    "        'MobileNetSSD_deploy.caffemodel in the same folder as this notebook.'\n",
    "    )\n",
    "else:\n",
    "    print(' Found model files.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495862b6-b19b-440b-858f-19094ab71803",
   "metadata": {},
   "source": [
    "## Class Labels\n",
    "- MobileNet SSD detects 20 PASCAL/VOC-like classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1ea9bb8-7cea-433a-a82d-555a65cc0099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n"
     ]
    }
   ],
   "source": [
    "CLASSES = [\n",
    "    'background','aeroplane','bicycle','bird','boat','bottle','bus','car','cat','chair',\n",
    "    'cow','diningtable','dog','horse','motorbike','person','pottedplant','sheep','sofa','train','tvmonitor'\n",
    "]\n",
    "COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))\n",
    "print('Classes:', CLASSES[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c960e3d-d192-45b5-a8e7-37e718730973",
   "metadata": {},
   "source": [
    "## Load Network and choose backend/target\n",
    "- By default, we use CPU. If OpenCV is built with CUDA, you can switch target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "557ff274-b8ee-4489-a157-e8f4aab34ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model loaded into OpenCV DNN.\n"
     ]
    }
   ],
   "source": [
    "net = cv2.dnn.readNetFromCaffe(PROTOTXT, CAFFEMODEL)\n",
    "# Try to set preferable backend/target (safe fallbacks)\n",
    "try:\n",
    "    net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "    net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "except Exception as e:\n",
    "    print('Note:', e)\n",
    "print(' Model loaded into OpenCV DNN.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2783c15-a1d7-4716-9744-fe9c43837f40",
   "metadata": {},
   "source": [
    "##  Quick Sanity Check on a Single Frame\n",
    "- Test on one image/frame to verify the pipeline and tune thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c723f1f-cec8-4556-88dd-ea305737ffa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper ready: detect_and_draw(frame, conf_thresh=0.4)\n"
     ]
    }
   ],
   "source": [
    "def detect_and_draw(frame, conf_thresh=0.4):\n",
    "    (h, w) = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 0.007843, (300, 300), 127.5)\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    \n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = float(detections[0, 0, i, 2])\n",
    "        if confidence < conf_thresh:\n",
    "            continue\n",
    "        idx = int(detections[0, 0, i, 1])\n",
    "        if idx >= len(CLASSES):\n",
    "            continue\n",
    "        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "        (startX, startY, endX, endY) = box.astype('int')\n",
    "        label = f\"{CLASSES[idx]}: {confidence:.2f}\"\n",
    "        color = COLORS[idx]\n",
    "        cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
    "        y = startY - 10 if startY - 10 > 10 else startY + 20\n",
    "        cv2.putText(frame, label, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    return frame\n",
    "\n",
    "print('Helper ready: detect_and_draw(frame, conf_thresh=0.4)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2bebfa-8139-4aed-a17c-f18251582ab5",
   "metadata": {},
   "source": [
    "## Run on Video\n",
    "- Reads `prasu_video.mp4`, runs detection per frame, and writes an annotated video output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "370c1734-debe-4ed4-a23c-272c2764681f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved annotated video → prasu_video_mobilenetssd.mp4\n",
      "Frames processed: 261, Processing FPS: 4.72\n"
     ]
    }
   ],
   "source": [
    "input_path = 'prasu_video.mp4'  \n",
    "output_path = 'prasu_video_mobilenetssd.mp4'\n",
    "CONF_THRESH = 0.4   # try 0.2 if you see missed detections\n",
    "\n",
    "cap = cv2.VideoCapture(input_path)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError('Could not open input video: ' + input_path)\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "fps = fps if fps and fps > 0 else 25  # fallback\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "t0 = time.time()\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    annotated = detect_and_draw(frame, CONF_THRESH)\n",
    "    writer.write(annotated)\n",
    "    frame_count += 1\n",
    "\n",
    "cap.release()\n",
    "writer.release()\n",
    "elapsed = time.time() - t0\n",
    "proc_fps = frame_count / elapsed if elapsed > 0 else 0\n",
    "\n",
    "print(f' Saved annotated video → {output_path}')\n",
    "print(f'Frames processed: {frame_count}, Processing FPS: {proc_fps:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4be3ab2-9ecc-4f98-b1e2-04bc5f17b787",
   "metadata": {},
   "source": [
    "- **MobileNet SSD Detections**:-` {0}  `\n",
    "- **MobileNet SSD FPS**:-` 4.72 `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8d2628-fca2-4c7c-ac63-4211d168c086",
   "metadata": {},
   "source": [
    "## MobileNet SSD\n",
    "\n",
    "1. Higher FPS (`4.7`), but detected nothing.\n",
    "\n",
    "2. Likely model or confidence threshold issue.\n",
    "\n",
    "3. Not usable in this run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bacf8ee-d71f-4969-9947-4254386df50b",
   "metadata": {},
   "source": [
    "# Model Comparison: YOLOv5 vs MobileNet SSD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e024521-d750-478d-a8a1-2ba1a40d012f",
   "metadata": {},
   "source": [
    " - In this notebook, we compare the performance of two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbcd8230-4706-4daf-aa29-7b09ea6d0760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a524ff-c437-49d9-a765-15f5f70669f0",
   "metadata": {},
   "source": [
    "## Model Results Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953353c4-2c9d-4a2a-b6b1-4a05cf90d8c8",
   "metadata": {},
   "source": [
    "- Store FPS and unique objects detected for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d88402d0-ac4f-4502-b99b-ec3924aa66f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>FPS</th>\n",
       "      <th>Unique Objects Detected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YOLOv5</td>\n",
       "      <td>2.82</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MobileNet SSD</td>\n",
       "      <td>4.72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model   FPS  Unique Objects Detected\n",
       "0         YOLOv5  2.82                        4\n",
       "1  MobileNet SSD  4.72                        0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = {\n",
    "    'Model': ['YOLOv5', 'MobileNet SSD'],\n",
    "    'FPS': [2.82, 4.72],\n",
    "    'Unique Objects Detected': [4, 0]\n",
    "}\n",
    "df = pd.DataFrame(results)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cb0af8-e376-4d9c-acbe-205a685ffb66",
   "metadata": {},
   "source": [
    "## FPS Comparison\n",
    "- FPS shows how fast the model processes frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79a89c98-fafc-449d-8786-905c5760c7ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(x='Model', y='FPS', data=df, palette='Blues_d')\n",
    "plt.title('Model FPS Comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5091818f-f204-4048-94c1-777b51daf717",
   "metadata": {},
   "source": [
    "##  Unique Objects Detected\n",
    "- Number of unique objects detected by each model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dc27f6-26ad-4d90-9332-506a978dca92",
   "metadata": {},
   "source": [
    "## Best Model Selection\n",
    "- We select the model that detected the most objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b17ed43a-d224-46dd-95ef-5827ece312a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Model based on objects detected:\n",
      "Model                      YOLOv5\n",
      "FPS                          2.82\n",
      "Unique Objects Detected         4\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "best_model = df.loc[df['Unique Objects Detected'].idxmax()]\n",
    "print(' Best Model based on objects detected:')\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329ee95e-49c3-45e0-9447-5bb522cc67dc",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- **YOLOv5** → Best accuracy, multiple objects detected (Recommended)\n",
    "- **MobileNet SSD** → Faster but failed to detect objects in this test\n",
    "\n",
    " **Final Best Choice = YOLOv5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9835140e-39c0-4be3-9302-7468cb4db2c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
